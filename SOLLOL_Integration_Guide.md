# SOLLOL Integration Guide for Hydra

## Quick Summary

**SOLLOL CAN replace these Hydra components**:
- âœ… `core/distributed.py` (DistributedManager) - 80% replacement
- âœ… `node_agent.py` - Can be removed (auto-discovery replaces it)
- âœ… `OllamaLoadBalancer` - 90% replacement (SOLLOL is better)

**SOLLOL CANNOT replace**:
- âŒ `core/orchestrator.py` (ModelOrchestrator)
- âŒ `workflows/dag_pipeline.py` (Prefect workflows)
- âŒ `core/memory.py` (Memory hierarchy)
- âŒ Task decomposition and synthesis logic

**Bottom Line**: Use SOLLOL as Hydra's distribution layer, not as a complete replacement.

---

## Step-by-Step Integration

### Phase 1: Preparation (1 day)

#### 1.1 Install SOLLOL
```bash
pip install sollol
```

#### 1.2 Verify SOLLOL Installation
```python
from sollol import OllamaPool, discover_ollama_nodes

# Test auto-discovery
nodes = discover_ollama_nodes()
print(f"Found {len(nodes)} Ollama nodes: {nodes}")

# Test pool creation
pool = OllamaPool.auto_configure()
print(f"Pool initialized with {len(pool.nodes)} nodes")
```

#### 1.3 Backup Current Code
```bash
cd /home/joker/hydra
git checkout -b sollol-integration
git add -A && git commit -m "Backup before SOLLOL integration"
```

### Phase 2: Core Integration (2-3 days)

#### 2.1 Modify `main.py`

**BEFORE:**
```python
from core.distributed import DistributedManager

distributed_manager = DistributedManager(nodes_config)
# Start background tasks
distributed_health_task = asyncio.create_task(distributed_manager.health_check_loop())
```

**AFTER:**
```python
from sollol import OllamaPool

# Initialize SOLLOL pool (auto-discovers nodes)
pool = OllamaPool.auto_configure(
    enable_intelligent_routing=True,
    routing_strategy='intelligent',
    enable_dask=True,  # Optional: for batch processing
    register_with_dashboard=True,  # Optional: for UI monitoring
    app_name='Hydra'  # Custom app name for dashboard
)

logger.info(f"ðŸš€ Initialized SOLLOL OllamaPool with {len(pool.nodes)} nodes")

# No need to start health check loop - SOLLOL manages it internally
```

#### 2.2 Update Prefect Tasks in `workflows/dag_pipeline.py`

**BEFORE:**
```python
@task(retries=3)
async def execute_subtask(subtask: Dict, model_pool: List[str]) -> Dict:
    from ..models.ollama_manager import OllamaLoadBalancer, ModelPool
    
    hosts = [...]
    lb = OllamaLoadBalancer(hosts)
    pool = ModelPool(lb, {'temperature': 0.7})
    
    responses = await pool.get_diverse_responses(
        subtask['subtask'],
        model_pool,
        max_concurrent=3
    )
```

**AFTER:**
```python
@task(retries=3)
async def execute_subtask(subtask: Dict, model_pool: List[str]) -> Dict:
    # Use the global SOLLOL pool instead
    from ..main import pool
    
    responses = []
    for model in model_pool:
        try:
            result = await pool.chat(
                model=model,
                messages=[{"role": "user", "content": subtask['subtask']}],
                priority=7  # Normal priority for code execution
            )
            responses.append({
                'model': model,
                'response': result['message']['content']
            })
        except Exception as e:
            logger.warning(f"Failed to get response from {model}: {e}")
    
    return {
        'subtask_id': subtask['id'],
        'prompt': subtask['subtask'],
        'responses': responses
    }
```

#### 2.3 Delete Distributed Files

```bash
# No longer needed with SOLLOL's auto-discovery
rm /home/joker/hydra/core/distributed.py
rm /home/joker/hydra/node_agent.py

# Update imports in main.py and other files
sed -i '/from core.distributed import/d' /home/joker/hydra/main.py
sed -i '/import node_agent/d' /home/joker/hydra/main.py
```

#### 2.4 Update `core/orchestrator.py` to Use Pool

```python
# Add pool parameter to ModelOrchestrator
class ModelOrchestrator:
    def __init__(self, load_balancer=None, pool=None, config_path: str = "config/models.yaml"):
        self.lb = load_balancer  # Keep for compatibility
        self.pool = pool  # New: use SOLLOL pool
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        self.light_model = self.config['orchestrators']['light']['model']
        self.heavy_model = self.config['orchestrators']['heavy']['model']
        self.json_pipeline = JSONPipeline(load_balancer)
    
    async def analyze_task(self, prompt: str, context: Dict = None) -> TaskComplexity:
        analysis_prompt = f"""..."""
        
        # Use pool if available, fall back to load_balancer
        if self.pool:
            response = await self.pool.chat(
                model=self.light_model,
                messages=[{"role": "user", "content": analysis_prompt}],
                priority=8  # High priority for orchestration tasks
            )
            result_text = response['message']['content']
        else:
            response = await self.lb.generate(
                model=self.light_model,
                prompt=analysis_prompt,
                temperature=0.3
            )
            result_text = response['response']
        
        complexity_str = result_text.strip().upper()
        return TaskComplexity.__members__.get(complexity_str, TaskComplexity.MODERATE)
```

#### 2.5 Update `main.py` Initialization

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Initializing Hydra system...")
    await db_manager.initialize()
    
    global load_balancer, orchestrator, memory, pool
    
    # Initialize SOLLOL pool (replaces distributed_manager)
    pool = OllamaPool.auto_configure(
        enable_intelligent_routing=True,
        enable_dask=True,
        register_with_dashboard=True,
        app_name='Hydra'
    )
    
    # Keep load_balancer for backward compatibility
    # It can delegate to pool if needed
    load_balancer = OllamaLoadBalancer(
        [f"http://{n['host']}:{n['port']}" for n in pool.nodes]
    )
    
    # Initialize orchestrator with pool
    orchestrator = ModelOrchestrator(load_balancer=load_balancer, pool=pool)
    memory = HierarchicalMemory(db_manager)
    
    # Start background tasks
    memory_migration_task = asyncio.create_task(memory.tier_migration())
    
    logger.info("Hydra system initialized successfully")
    
    yield
    
    # Shutdown
    memory_migration_task.cancel()
    await db_manager.close()
    logger.info("Hydra system shutdown complete")
```

### Phase 3: Testing (2-3 days)

#### 3.1 Unit Tests

```python
# tests/test_sollol_integration.py

import pytest
from sollol import OllamaPool, discover_ollama_nodes
from app import initialize_system

@pytest.mark.asyncio
async def test_pool_initialization():
    """Test SOLLOL pool initialization"""
    pool = OllamaPool.auto_configure()
    assert len(pool.nodes) > 0
    logger.info(f"âœ… Pool initialized with {len(pool.nodes)} nodes")

@pytest.mark.asyncio
async def test_node_discovery():
    """Test auto-discovery of Ollama nodes"""
    nodes = discover_ollama_nodes()
    assert len(nodes) > 0
    for node in nodes:
        assert 'host' in node
        assert 'port' in node
    logger.info(f"âœ… Discovered {len(nodes)} nodes")

@pytest.mark.asyncio
async def test_chat_request():
    """Test basic chat request through pool"""
    pool = OllamaPool.auto_configure()
    response = await pool.chat(
        model="llama3.2",
        messages=[{"role": "user", "content": "Hello!"}],
        priority=5
    )
    assert 'message' in response
    assert 'content' in response['message']
    logger.info(f"âœ… Chat request successful: {response['message']['content'][:50]}...")

@pytest.mark.asyncio
async def test_model_orchestrator_with_pool():
    """Test ModelOrchestrator using pool"""
    pool = OllamaPool.auto_configure()
    orchestrator = ModelOrchestrator(pool=pool)
    
    complexity = await orchestrator.analyze_task(
        "Create a Python function for fibonacci",
        {}
    )
    assert complexity in ['SIMPLE', 'MODERATE', 'COMPLEX']
    logger.info(f"âœ… Task analysis: {complexity}")

@pytest.mark.asyncio
async def test_prefect_workflow_with_sollol():
    """Test Prefect workflow using SOLLOL pool"""
    from workflows.dag_pipeline import code_generation_pipeline
    
    result = code_generation_pipeline(
        prompt="Create a web server in Python",
        context={}
    )
    # Verify result structure
    assert 'synthesized_code' in result
    logger.info(f"âœ… Workflow completed successfully")
```

#### 3.2 Integration Tests

```python
# tests/test_integration.py

@pytest.mark.asyncio
async def test_full_code_generation_pipeline():
    """End-to-end test of code generation with SOLLOL"""
    client = TestClient(app)
    
    response = client.post('/generate', json={
        'prompt': 'Create a simple HTTP server',
        'context': {},
        'temperature': 0.7
    })
    
    assert response.status_code == 200
    data = response.json()
    assert 'synthesized_code' in data
    assert len(data['synthesized_code']) > 0
    logger.info(f"âœ… Full pipeline test passed")

@pytest.mark.asyncio
async def test_priority_queuing():
    """Test SOLLOL's priority queuing"""
    pool = OllamaPool.auto_configure()
    
    # Submit low priority requests
    low_priority_tasks = [
        pool.chat(model="llama3.2", 
                 messages=[{"role": "user", "content": f"Task {i}"}],
                 priority=1)
        for i in range(5)
    ]
    
    # Submit high priority request
    high_priority_task = pool.chat(
        model="llama3.2",
        messages=[{"role": "user", "content": "High priority task"}],
        priority=10
    )
    
    # High priority should complete faster
    # (In real test, measure timing)
    logger.info(f"âœ… Priority queuing test completed")

@pytest.mark.asyncio
async def test_failover():
    """Test SOLLOL's failover mechanism"""
    pool = OllamaPool.auto_configure()
    
    # Should handle node failures gracefully
    response = await pool.chat(
        model="llama3.2",
        messages=[{"role": "user", "content": "Test failover"}],
        priority=5
    )
    
    assert 'message' in response
    logger.info(f"âœ… Failover test passed")
```

#### 3.3 Run Tests

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_sollol_integration.py -v

# Run with coverage
pytest tests/ --cov=core --cov=workflows --cov-report=html
```

### Phase 4: Deployment (1 day)

#### 4.1 Deploy to Development Environment

```bash
# Build and test
python -m pytest tests/

# Start Hydra with SOLLOL
python main.py

# In another terminal, start SOLLOL dashboard (optional)
python -m sollol.dashboard_service --port 8080
```

#### 4.2 Verify Monitoring

```bash
# Check SOLLOL dashboard
# Open: http://localhost:8080

# Check Hydra API
curl http://localhost:8000/health

# Check SOLLOL health
curl http://localhost:8000/health
```

#### 4.3 Monitor Metrics

```bash
# Check node discovery
curl http://localhost:8000/nodes

# Check routing decisions
curl http://localhost:8000/routing

# Check performance metrics
curl http://localhost:8000/metrics
```

#### 4.4 Production Deployment Checklist

- [ ] All tests passing
- [ ] Node discovery working
- [ ] Health checks passing
- [ ] Dashboard accessible
- [ ] Metrics being logged
- [ ] Failover tested
- [ ] Load tested with typical workload
- [ ] Documentation updated
- [ ] Team trained on new system

---

## Configuration Reference

### SOLLOL Pool Configuration

```python
pool = OllamaPool.auto_configure(
    # Routing strategy
    enable_intelligent_routing=True,
    routing_strategy='intelligent',  # 'intelligent', 'round_robin', 'least_loaded'
    
    # Node discovery
    exclude_localhost=False,
    discover_all_nodes=False,  # Set to True for comprehensive network scan
    
    # Execution backends
    enable_ray=False,           # For actor-based parallelism
    enable_dask=True,           # For batch processing
    dask_address=None,          # Auto-connect or start local
    
    # GPU monitoring
    enable_gpu_redis=True,      # Requires GPU reporter on nodes
    redis_host='localhost',
    redis_port=6379,
    
    # Caching
    enable_cache=True,
    cache_max_size=1000,
    cache_ttl=3600,  # 1 hour
    
    # Observability
    register_with_dashboard=True,
    app_name='Hydra'            # Custom application name
)
```

### Environment Variables

```bash
# Node discovery
export OLLAMA_HOST="http://192.168.1.10:11434"
export SOLLOL_DISCOVER_ALL_NODES="false"
export SOLLOL_AUTO_DISCOVER_ENABLED="true"

# GPU monitoring
export SOLLOL_ENABLE_GPU_REDIS="true"
export SOLLOL_GPU_REDIS_HOST="localhost"
export SOLLOL_GPU_REDIS_PORT="6379"

# Dashboard
export SOLLOL_DASHBOARD="true"
export SOLLOL_DASHBOARD_PORT="8080"

# Metrics
export SOLLOL_METRICS_ENABLED="true"
export SOLLOL_INFLUXDB_HOST="localhost"
export SOLLOL_INFLUXDB_PORT="8086"

# Routing
export SOLLOL_MIN_VRAM_MB="1000"  # Fallback to CPU if GPU VRAM < 1GB
export SOLLOL_ROUTING_STRATEGY="intelligent"

# Performance tuning
export SOLLOL_MAX_CONCURRENT_REQUESTS="100"
export SOLLOL_REQUEST_TIMEOUT_SECONDS="300"
```

---

## Troubleshooting

### Issue: SOLLOL Not Finding Nodes

**Symptoms**: `No Ollama nodes discovered`

**Solution**:
```bash
# 1. Verify Ollama is running on nodes
ssh node1 "curl http://localhost:11434/api/tags"

# 2. Test network connectivity
ping node1
nmap -p 11434 node1

# 3. Enable full network scan
pool = OllamaPool.auto_configure(discover_all_nodes=True)

# 4. Manually specify nodes if auto-discovery fails
pool = OllamaPool(nodes=[
    {'host': '192.168.1.10', 'port': '11434'},
    {'host': '192.168.1.11', 'port': '11434'}
])
```

### Issue: Health Checks Failing

**Symptoms**: Nodes marked as unhealthy

**Solution**:
```python
# Check SOLLOL health check settings
pool = OllamaPool.auto_configure()

# Verify node health manually
for node in pool.nodes:
    status = pool.health_monitor.check_node(node)
    print(f"{node['host']}: {status}")

# Increase health check timeout
export SOLLOL_HEALTH_CHECK_TIMEOUT="30.0"
```

### Issue: Slow Routing Decisions

**Symptoms**: 50ms+ per request

**Solution**:
```python
# Enable caching
pool = OllamaPool.auto_configure(
    enable_cache=True,
    cache_max_size=1000
)

# Use simpler routing strategy
pool = OllamaPool.auto_configure(
    routing_strategy='round_robin'
)

# Monitor routing performance
pool.routing_logger.export_stats()
```

### Issue: Dashboard Not Starting

**Symptoms**: Cannot connect to dashboard

**Solution**:
```bash
# Check if port is in use
lsof -i :8080

# Use different port
python -m sollol.dashboard_service --port 9090

# Check logs
tail -f /var/log/sollol/dashboard.log

# Verify Redis connection (if using GPU monitoring)
redis-cli ping
```

---

## Migration Checklist

- [ ] **Phase 1: Preparation**
  - [ ] Install SOLLOL
  - [ ] Verify installation
  - [ ] Backup code

- [ ] **Phase 2: Integration**
  - [ ] Update `main.py` with OllamaPool
  - [ ] Update Prefect tasks
  - [ ] Delete `core/distributed.py`
  - [ ] Delete `node_agent.py`
  - [ ] Update imports
  - [ ] Update ModelOrchestrator

- [ ] **Phase 3: Testing**
  - [ ] Write unit tests
  - [ ] Write integration tests
  - [ ] Test failover
  - [ ] Test priority queuing
  - [ ] Load test
  - [ ] Coverage analysis

- [ ] **Phase 4: Deployment**
  - [ ] Deploy to dev
  - [ ] Verify functionality
  - [ ] Check monitoring
  - [ ] Deploy to staging
  - [ ] Deploy to production
  - [ ] Monitor for issues

- [ ] **Post-Deployment**
  - [ ] Update documentation
  - [ ] Train team
  - [ ] Establish runbooks
  - [ ] Set up alerting
  - [ ] Monitor metrics

---

## Performance Benchmarks

Expected improvements after SOLLOL integration:

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Routing time | 50ms | 5ms | 10x faster |
| Failover time | 2+ min | 10-30s | 5-12x faster |
| Node discovery | Manual | Auto | Automation |
| Dashboard latency | N/A | <100ms | Real-time |
| Throughput | 2k req/min | 4k req/min | 2x higher |
| GPU utilization | 60% | 85% | +25% |

---

## Support & Documentation

**SOLLOL Resources**:
- GitHub: https://github.com/BenevolentJoker-JohnL/SOLLOL
- Documentation: /home/joker/SOLLOL/README.md
- Architecture: /home/joker/SOLLOL/ARCHITECTURE.md
- Configuration: /home/joker/SOLLOL/CONFIGURATION.md

**Integration Support**:
- Review SOLLOL_vs_Hydra_Analysis.md for detailed comparison
- Check example integration in workflows/dag_pipeline.py
- Run test suite for verification

